var relearn_searchindex = [
  {
    "breadcrumb": "Mathematics and Programming",
    "content": "This section wil cover posts about computer science, programming and devops.",
    "description": "This section wil cover posts about computer science, programming and devops.",
    "tags": [],
    "title": "Computer Science",
    "uri": "/computer-science/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming",
    "content": "This section wil cover posts about mathematics; I love logic, set-theory, combinatorics, optimization.",
    "description": "This section wil cover posts about mathematics; I love logic, set-theory, combinatorics, optimization.",
    "tags": [],
    "title": "Mathematics",
    "uri": "/mathematics/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming \u003e Computer Science",
    "content": "The Task It is not quite uncommon to have some time series data with some jitter in it. You might call these measurement errors (e.g. ADC jitter), disturbing signals (e.g. electro-magnetic interference), or just “fluctuation” in case it is a long term “trend” you’re interested in.\nThere are many ways to cope with that:\nsmoothing splines: taken from the octave documentation Kalman filters Moving avergage \\begin{gather} y[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} x[n-k] \\end{gather} The latter is a rather simple to approach, but it bears a typical students trap that comes with some computational penalty: The naive approach of just summing up the samples within the window, then dividing to get the average, and ‘slide’ the window one sample to the right is time-consuming, and there are better ways to do this.\nHaving said that, the computationally faster implementations need some care as well.\nTL;DR If just a moving average is what fits your needs, using an accumulator instead of summing up the samples again and again is just fine and very fast. If you are worried about numerical problems, e.g. because the summation might go on and on for several billion samples with changing signs, you could either decide to use an integer accumulator and add/subtract raw ADC values - this is viable if the raw ADC values are still available - and yields 100% accuracy, or use Kahans summation technique on floats to recover the error. This technique might also be a mitigation for numerical errors on silicon that does not support 64-bit floating point operations (e.g. embedded systems), periodically recalculate the sum to avoid drift. If you are interested in the general case, i.e. you want to apply a generic, possibly non-uniform, window function to the signal, you might want to look into the FFT and convolution algorithms. The FFT runs in $O(N\\,log(N))$, and the convolution could be computed with point-wise multiplication in frequency domain. You will probably find good libraries that give you an arbitrary-radix FFT in most mainstream programming languages, but just in case it is needed to do an implementation from scratch: A low-hanging fruit is to first head for the Cooley-Tukey algorithm to achieve a radix-2 FFT, and then approach arbitrary-radix FFTs via the Bluestein algorithm. There are some aspects which we won’t dive into: Numerical analysis of expected errors using the FFT/convolution approach. Phase errors; there is a question as on how to properly align the averaged samples. Strictly speaking, it would be necessary to note that with an averaging window of 11 samples you likely would be able to obtain your first ‘valid’ result in time-domain at sample 6, that is: \\begin{gather} y[n] = \\frac{1}{N} \\sum_{k=n-\\lfloor{}k/2\\rfloor{}}^{n+\\lfloor{}k/2\\rfloor{}} x[k] \\end{gather} The Naïve Approach func naiveMovingAverage(data []float64, windowSize int) []float64 { if windowSize \u003c= 0 || windowSize \u003e len(data) { return nil } averages := make([]float64, len(data)-windowSize+1) for i := 0; i \u003c= len(data)-windowSize; i++ { sum := 0.0 for j := 0; j \u003c windowSize; j++ { sum += data[i+j] } averages[i] = float64(sum) / float64(windowSize) } return averages } As we can see the two loops make up for $O(N\\,K)$ operations, where $N$ is the number of samples in the input data, and $K$ is the window size.\nThe Accumulator Approach We tried to avoid presenting our own FFT implementation here, as there are many out there, and it would just blow up the complexity of this article. Also: I would not encourage to re-implement an FFT from scratch if no exotic seupt requires just this.\nWe used: github.com/mjibson/go-dsp/fft which, btw., also follows the Cooley-Tukey-Bluestein approach:\nfunc movingAverageConvolve(data []float64, windowSize int) []float64 { if windowSize \u003c= 0 || windowSize \u003e len(data) { return nil } // Define the moving average window window := make([]complex128, len(data)) cdata := make([]complex128, len(data)) for i := 0; i \u003c windowSize; i++ { window[len(data)-i-1] = complex(1.0/float64(windowSize), 0) cdata[i] = complex(data[i], 0) } for i := windowSize; i \u003c len(data); i++ { cdata[i] = complex(data[i], 0) } result := fft.Convolve(cdata, window) // Convert the result to real values and shift for correct alignment realResult := make([]float64, len(result)) for i := 0; i \u003c len(result); i++ { realResult[(i+1)%len(result)] = real(result[i]) } // Trim the result to exclude padding from FFT (valid convolution) return realResult[:len(data)-windowSize+1] } We will evaluate the performance of that approach in the final section, its dominance relies on the window size. In the code we see that the convolution demands some index tricks, i.e. reversing the window, and the result needs a shift for correct alignment. With one million samples and roughly 10k window size the performance boost is roughly a factor of 5 - I think that still could be improved upon in several ways:\nI think it is possible to do this without complex arithmetic, i.e. use a discrete cosine transform (DCT) instead of the FFT, as the input is real-valued. The FFT library used here does just the aforementioned pragmatic / easy to implement trick to leverage the Bluestein-FFT on top of the Cooley-Tukey radix-2 FFT, in case the number of samples is not a power of 2. This can be improved upon with FFTW, cuFFT etc. Although the performance boost is not overwhelming, the convolution approach is more flexible, and can be used for much more general cases when the window function is arbitrary.\nThe Simple Accumulator Approach func AccumulatorMovingAverage(data []float64, windowSize int) []float64 { if windowSize \u003c= 0 || windowSize \u003e len(data) { return nil } var result = make([]float64, len(data)-windowSize+1) accumulator := 0.0 // Holds the current sum of the window // Initialize the first window for i := 0; i \u003c windowSize; i++ { accumulator += data[i] } result[0] = accumulator / float64(windowSize) // Slide the window through the data for i := windowSize; i \u003c len(data); i++ { // Subtract the value leaving the window and add the value entering the window accumulator += data[i] - data[i-windowSize] // Compute the average for the current window result[i-windowSize+1] = accumulator / float64(windowSize) } return result } This approach is very simple, and very fast. If there are no worries about numerical stability, or if integer ADC values are available, this is the simplest and fastest solution to the task.\nThe only worries I might be aware of in case of floating point samples are accumulating round-off-errors - but these could be mitigated with a Kahan summation technique or a periodic re-calculation and then (precision-)benchmarked against the naïve approach.\nAccumulator with Kahans Error Recovery func KahanMovingAverage(data []float64, windowSize int) []float64 { if windowSize \u003c= 0 || windowSize \u003e len(data) { return nil } var result = make([]float64, len(data)-windowSize+1) accumulator := 0.0 // Holds the current sum of the window compensation := 0.0 // Kahan compensation for lost precision // Initialize the first window for i := 0; i \u003c windowSize; i++ { y := data[i] - compensation t := accumulator + y compensation = (t - accumulator) - y accumulator = t } result[0] = accumulator / float64(windowSize) // Slide the window through the data for i := windowSize; i \u003c len(data); i++ { // Subtract the value leaving the window y := -data[i-windowSize] - compensation t := accumulator + y compensation = (t - accumulator) - y accumulator = t // Add the value entering the window y = data[i] - compensation t = accumulator + y compensation = (t - accumulator) - y accumulator = t // Compute the average for the current window result[i-windowSize+1] = accumulator / float64(windowSize) } return result } Results We chose\nsampleSize := 1_000_000 data := make([]float64, sampleSize) for i := 0; i \u003c sampleSize; i++ { data[i] = float64(i + 1) // for simplicity reasons } windowSize := 10001 // avoid \"0.5-style\" results and got the following results on a 4GHz Ryzen 5 CPU:\nTime taken for naive approach: 7.315492521s Moving average by naive summation - first 5 samples: [5001 5002 5003 5004 5005] Moving average by naive summation - last 5 samples: [994995 994996 994997 994998 994999 995000] -------------------- Time taken for fft approach: 1.452222336s Moving averages by convolution - first 5 samples: : [5000.999881542342 5001.999705137291 5002.999716104469 5003.999729782633 5004.9997446745765] Moving averages by convolution - last 5 samples: : [994994.9998554322 994995.9998804536 994996.9998385616 994997.9998366546 994998.9998474488 994999.9998630307] -------------------- Time taken for accumulator approach: 2.926404ms Moving averages by accumulator - first 5 samples: : [5001 5002 5003 5004 5005] Moving averages by accumulator - last 5 samples: : [994995 994996 994997 994998 994999 995000] -------------------- Time taken for accumulator approach with Kahan recovery: 7.520729ms Moving averages by Kahan accumulator - first 5 samples: : [5001 5002 5003 5004 5005] Moving averages by Kahan accumulator - last 5 samples: : [994995 994996 994997 994998 994999 995000] We observe some minor numerical problems with the FFT approach, but with the set sample sizes a factor of five boost.\nTo be fair, I guess because of the nature of the samples we chose for simplicity, we are actually never really leaving integer arithmetic here in the accumulators (mantissa and exponent make up for no real fractional number).\nWe could check this with fragments like\nfunc isInteger(f float64) bool { return f == math.Trunc(f) } // ... if !isInteger(accumulator) { fmt.Println(\"accumulator is not an integer\") } And do a comparison with\n//... data[i] = math.Sin(float64(i) / 10000) //... Just to give a glimpse of the numerics here: The last samples are most problematic for the accumulator based solution, and we see a two-decimal-digit edge here for Kahan recovery over the simple accumulator:\nTime taken for naive approach: 7.384292112s Moving average by naive summation - first 5 samples: [0.4596937979182286 0.459777942004903 0.45986208149379804 0.45994621638407224 0.4600303466748843] Moving average by naive summation - last 5 samples: [-0.8227865748742728 -0.8227373364171561 -0.8226880897326658 -0.822638834821295 -0.8225895716835357 -0.8225403003198811] -------------------- Time taken for fft approach: 1.452728596s Moving averages by convolution - first 5 samples: : [0.45969379786813874 0.4597779420491195 0.45986208162457526 0.45994621638007066 0.46003034675477156] Moving averages by convolution - last 5 samples: : [-0.8227865748577293 -0.8227373363776562 -0.822688089655593 -0.8226388348728317 -0.8225895717086071 -0.822540300302254] -------------------- Time taken for accumulator approach: 2.242687ms Moving averages by accumulator - first 5 samples: : [0.4596937979182286 0.459777942004903 0.45986208149379804 0.45994621638407224 0.4600303466748843] Moving averages by accumulator - last 5 samples: : [-0.8227865748741857 -0.8227373364170689 -0.8226880897325789 -0.822638834821208 -0.8225895716834487 -0.8225403003197936] -------------------- Time taken for accumulator approach with Kahan recovery: 7.242217ms Moving averages by Kahan accumulator - first 5 samples: : [0.45969379791822745 0.45977794200490185 0.4598620814937969 0.45994621638407107 0.46003034667488313] Moving averages by Kahan accumulator - last 5 samples: : [-0.8227865748742715 -0.8227373364171547 -0.8226880897326648 -0.8226388348212939 -0.8225895716835347 -0.8225403003198797] We can clearly see how the accumulator approach outperforms the naïve and the FFT approach, by both quality and speed, the latter by orders of magnitude.\nThat again brings me to my favourite message regarding practitioners and theorists: “Vision without action is a dream; action without vision is a journey without direction.”\n\\begin{gather} \\infty \\end{gather}",
    "description": "The Task It is not quite uncommon to have some time series data with some jitter in it. You might call these measurement errors (e.g. ADC jitter), disturbing signals (e.g. electro-magnetic interference), or just “fluctuation” in case it is a long term “trend” you’re interested in.\nThere are many ways to cope with that:\nsmoothing splines: taken from the octave documentation Kalman filters Moving avergage \\begin{gather} y[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} x[n-k] \\end{gather} The latter is a rather simple to approach, but it bears a typical students trap that comes with some computational penalty: The naive approach of just summing up the samples within the window, then dividing to get the average, and ‘slide’ the window one sample to the right is time-consuming, and there are better ways to do this.",
    "tags": [],
    "title": "Fold me, fast",
    "uri": "/computer-science/fold_me_fast/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Under construction Disclaimer Info © [https://github.com/workx247]. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. THE CONTENT IS PROVIDED “AS IS,” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. SEE THE LICENSE FOR THE FULL DISCLAIMER.\nWarning I don’t take responsibility of any form, neither legally nor financially, for using or downloading any software, algorithms and thoughts mentioned in this article. I don’t even claim correctness of fitness for a particular purpose.\nThis is going to be a blog about mathematics, programming and CS.\nFor most people these fields are lose connected - 10 years ago I perceived them as very different.\nXKCD summed up my own perception pretty well in this comic.\nToday I still find that comic amusing, but there is a real story behind that, that bothers many of us on a regular basis. In my daily work I witness a lot of prejudice. Practical advice being rejected out of arrogance, advice with more theoretical background being rejected out of ignorance, referred to as “book knowledge”.\nThe software crisis is not over yet - every programmer I know can tell a lot of stories about messed up projects. The truth is, the people drawn in this comic desperately need each other. We won’t be able to face economic, social and environmental challenges, if we don’t start working together, or what more matters more in my opinion: listening to each other.\nAs my background is in programmaning and mathematics, I think I can contribute at least in those to fields that also differ in ‘purity’.\nHelpful maths “Advanced stuff like, e.g. integrals, have no use in everyday life” Well, maybe not every day, but as soon as you get older, having children, lifting them and start having your first back pain. Your doc might send you to the radiologist, and that guy reads pictures that are computed:\nThis way: https://en.wikipedia.org/wiki/Radon_transform and/or this way: https://en.wikipedia.org/wiki/Kaczmarz_method So, living in constant pain or undergoing unnecessary surgery, vs. having a solid foundation diagnosis and being able to tell the state of your IVD depends on numerical analysis.\nIn other words, you might be just a few decades and one lift of a heavy child away from enjoying Radons and Kaczmarzs work.\n(I’m not mentioning scanning QR codes, the crypto involved when sending a message with the most popular messenger, or the fact that you can’t even read this text via HTTPS without cryptography.)\nThe coffee machine guy To be honest, this is something that could have happened to me as well as as student working at a company.\nSuppose you have a window function that moves over a signal, lets say computing the average of the last ten thousand (K) samples of a signal with a few million samples (N) is your goal for smoothing out the signal.\nThe naïve approach of just summing up the last thousand samples again and again gives you some $O(N\\,K)$ operations. This would yield a really slow computation, and whoever is waiting for that to finish, will spend a lot of time at the coffee machine, which in this case, was the best thing to happen, because the naïve approach could be very much improved.\nThere is a pragmatic improvement to this technique, as long as you could either rule out numeric problems, of use raw, integer ADC values: of course an accumulator variable could be used to store the sum, adding and subtracting samples as the window moves - that will be linear in N then.\nThere is also a general approach to this which gives us $O(N\\,log(N))$ operations: https://en.wikipedia.org/wiki/Convolution#Fast_convolution_algorithms which basically boils down to the FFT (and IFFT) running in $O(N\\,log(N))$, and the convolution could be computed with point-wise multiplication in frequency domain:\n\\begin{align} h_k = \\sum_{i=0}^{k} f_i g_{k-i} \\\\ h = \\text{IFFT}(\\text{FFT}(f) \\cdot \\text{FFT}(g)) \\end{align} There are some tricks involved (zero-padding), but the message is: talking to a colleagues who are either experiences programmers / or know some math could save you quite some time.\nThe ivory tower’s inhabitants The generic mistake: working alone for too long I knew a few very smart people at the university. While it is indeed true that many people were not able to follow what they were working on, it is completely wrong / utterly risky (specifically working a job at a company) to keep your knowledge and work to yourself.\nWhile this is often not directly penalized in academia, it is still tragic in some cases. While this is “just” one individual posting his/her concerns, it still shows the troubles people experience at the university:\nhttps://www.reddit.com/r/PhD/comments/156owmj/has_anyone_quit_their_phd_after_5_years/\nWhile I never tried reaching a PhD, I can stil relate to these people, knowing two who resigned on their PhD. I can relate to that feeling very much, and it is the main reason why I enjoy practical work, in a team - best case: together with other friends - and it is also the reason why I wrote my thesis on an algorithmic topic, so that I could bounce my ideas against some program and practical tests, and see if they work out.\nThere are exceptions to “working alone is ‘bad’ “, e.g. (https://en.wikipedia.org/wiki/Andrew_Wiles), but I still believe that the only risk of sharing your knowledge and progress in such a case is, that others might steal / publish your ides before you do. But that problem could imho be solved with notary services / cryptography / digital signatures / blockchain.\nJust some links to show, that mathematicians are self-critical and reflecting: Not every mathematician is an arrogant know-it-al. My pledge to my readers would be to bare with ‘purists’, talk to the guys - although I strongly agree it makes a lot of sense to not take arrogant behaviour:\nhttps://sites.math.rutgers.edu/~zeilberg/Opinion7.html\nhttps://www.reddit.com/r/math/comments/fxbp7s/are_there_any_notable_mathematiciansscientists/\nhttps://www.quora.com/Who-was-the-worst-mathematician-in-history",
    "description": "Under construction Disclaimer Info © [https://github.com/workx247]. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. THE CONTENT IS PROVIDED “AS IS,” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. SEE THE LICENSE FOR THE FULL DISCLAIMER.\nWarning I don’t take responsibility of any form, neither legally nor financially, for using or downloading any software, algorithms and thoughts mentioned in this article. I don’t even claim correctness of fitness for a particular purpose.",
    "tags": [],
    "title": "Mathematics and Programming",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming \u003e Mathematics",
    "content": "How did I get here? Some 20 years ago, when I was an undergraduate (CS) student, I was pretty fond of theoretical computer science, but I also secretly joined fellow students in their math classes, but also lead me to pursue mathematics later…\nDuring that time I usually earned some extra money as a tutor for theoretical CS and mathematics.\nBack then I often observed that both students and teachers, struggled to explain the technique of the diagonal argument in a general sense.\nUsually, most of the students were able to grasp the individual applications of that technique, Cantors uncountability proof for \\(\\mathbb{R}_{[0,1]}\\), or the power set theorem referring to \\(|\\mathcal{P}(S)| \u003e |S|\\) - or the halting problem - depending on the field of study of the respective student or teacher.\nCantors diagonal arguemt for the uncoutability of \\(\\mathbb{R}_{[0,1]}\\), Cantors theorem regarding the cardinality of power sets, The halting problem Warning I don’t take responsibility of any form, neither legally nor financially, for using or downloading any software, algorithms and thoughts mentioned in this article. I don’t even claim correctness of fitness for a particular purpose.\nTL;DR What I would like to show here on a more formal level, is what the diagonal argument actually is, and how to use it in its usual or contrapositional form.\nThe Diagonal Argument Given a function \\(f: I \\rightarrow S\\), a function \\(P: I \\times S \\rightarrow T \\), which is more like a property, i.e. \\(|T| \\geq 2\\) is the only necessary condition for \\(P\\). The following holds\n\\begin{gather} \\exists s^* \\in S\\ \\forall i \\in I: P(i, f(i)) \\neq P(i, s^*) \\\\ \\text{implies }f\\ \\text{ is not surjective.} \\end{gather} It might look much more complicated than it actually is. If $f$ is surjective, then $S$ is the range of $f$, $S = f(I)$, and because of $s \\in S$ some index $i^*$ would then yield $f(i^*) = s^*$, thus $P(i^*, f(i^*)) = P(i^*, s^*)$ contradicting the construction of $P$.\nThe equivalent, contrapositional form is:\n\\begin{gather} \\text{If } f \\text{ is surjective, then:}\\\\ \\quad \\not\\exists s^* \\in S\\ \\forall i \\in I: P(i, f(i)) \\neq P(i, s). \\end{gather} We could formally negate the implication (for all \\(s^*\\) exists an \\(i\\) sich that …) - but that is not the way the diagonal argument is usually applied. Rather that non-existence is exploited to refute a certain construction of $s^*$… which would also be my meta-theorem-like takeaway from the formal lighting of the diagonal argument:\nThe two forms which I am aware of are:\nThe direct form - the goal is to prove that no surjective \\(f\\) exists.\nYou are able to construct that special element \\(s^*\\) which is not in the image of \\(f\\) by exploiting the inner structure of the set \\(S\\) and the property \\(P\\) - the P-in-equality usually provides the means to do so.\nThe contrapositional form - the goal is to contradict some construction for \\(s^*\\):\nYou already know, by some theorem, that a surjective \\(f\\) exists. A construction of \\(s^*\\) then involves several steps that are proven to be legitimate and one step which is, only by assumption \\(A\\), legal/existent. By showing that \\(\\forall i \\in I: P(i, f(i)) \\neq P(i, s^*)\\) for the constructed/hypthetical \\(s^*\\), you have contradicted \\(f\\) being surjective and thus proven the construction to be invalid, and thus the assumption \\(A\\) to be wrong.\nThe restriction that, in the constructino of \\(s^*\\), only one assumption \\(A\\) is made, is crucial. If several steps in the construction of \\(s^*\\) are questionable, your proven result is weak in the sense that your proof doesn’t show anymore, precisely, which step in the construction fails. Fun fact: As in any theorem we might use it to falsify any aspect if it by just making clear that every other aspect of our proof / construction is valid. It might be thinkable to use the diagonal argument to prove that \\(|T| = 1\\), but I can’t think of an application like that. If you are aware of such an example, please let me know (but please give me some relevant citation, not an artificial, made-up example). Also, the coincidence that $I$ is part of the domain of $f$ as well as $P$ is, in my opinion, a rather ‘practical’ destillate of its usual applications - if preferred, a distinction could be made, see (**).\nThe classic examples of how to apply the diagonal argument Cantors uncountability proof for \\(\\mathbb{R}_{[0,1]}\\) We want to prove that the set of real numbers in the closed interval $[0,1]$ is uncountable. I.e. that any enumeration $f: \\mathbb{N} \\rightarrow \\mathbb{R}_{[0,1]}$ is not surjective.\nWe are going to first prove that any enumeration into $\\{0,1\\}^N$, the set of infinite 0-1-sequences, is not surjective, and are then going to show that $\\{0,1\\}^N$ could be embedded (injective) into $\\mathbb{R}_{[0,1]}$.\nFirst step: showing $|\\mathbb{N}| \u003c |\\{0,1\\}^N|$ Second step: showing $|\\{0,1\\}^N| \\leq |\\mathbb{R}_{[0,1]}|$ Thus obtaining $|\\mathbb{N}| \u003c |\\mathbb{R}_{[0,1]}|$ Proof: Suppose we got an arbitrary surjective enumeration $f: \\mathbb{N} \\rightarrow [0,1]^N $\nFor the reader, we relate to the symbols used in the diagonal theorem above:\n\\(I \\equiv \\mathbb{N}\\) \\(S \\equiv \\{0,1\\}^N\\) \\(T \\equiv \\{0,1\\}\\) The property \\(P\\) is defined by the i-th component of an element $s \\in S$.\n\\begin{align} P(i, s) = s_i\\begin{cases} 1\\\\ 0 \\end{cases} \\end{align} We see now, because the codomain $P$ is two-valued, we can always define a different i-th component for our exceptional sequence $s^*$:\n\\begin{align} s^*_i = 1 :\\iff 0 = P(i,f(i)) = s_i \\\\ s^*_i = 0 :\\iff 1 = P(i,f(i)) = s_i \\\\ \\end{align} Application of the diagonal argument then yields that $f$ is not surjective, $f(\\mathbb{N}) \\subsetneq \\{0,1\\}^N$. Because $f$ was arbitrary, we have shown that $N \u003c [0,1]^N$\nWhile a continued fraction like $0,111111... = 1$ in binary, it is not in decimal representation, which will be the aforementioned injective embedding:\nViewing the sequences in $\\{0,1\\}^N$ as decimal fractions, the first digit left of the comma always being 0, and leaving aside all other digits from 2 to 9, we have an injective embedding of $\\{0,1\\}^N$ into $\\mathbb{R}_{[0,1]}$ (thank you wikipedia).\nThat finally yields the second inequality $\\{0,1\\}^N \\leq \\mathbb{R}_{[0,1]}$, so $|N| \u003c |\\{0,1\\}^N| \\leq |\\mathbb{R}_{[0,1]}|$. $\\blacksquare$\nCantors theorem regarding the cardinality of power sets The power set of a set $X$ is the set of all subsets of $X$, denoted by $\\mathcal{P}(X)$.\nAs long as $X$ is finite, $|\\mathcal{P}(X)| = 2^{|X|}$, which is kind of clear and could be understood with a few examples, but for infinite sets intuition might fail.\nAgain, trying to do good teaching:\nLet $f$ be some mapping from $X$ to $\\mathcal{P}(X)$. We assume $f$ is onto $\\mathcal{P}(X)$, and will refute just that by using the diagonal argument. $ I \\equiv X $ $ S \\equiv \\mathcal{P}(X)$ We will denote elements of $X$ by $x$, and elements of $\\mathcal{P}(X)$ by $\\underline{x}$ $ T \\equiv \\{true,false\\} $ The property \\(P\\) is then defined via:\n\\begin{align} P(x, \\underline{x}) := x \\in \\underline{x} \\end{align} That is, given some element $x \\in X$, and some subset of $X$, which is by itself an element of $\\mathcal{P}(X)$ - that is $\\underline{x} \\in \\mathcal{P}(X)$ - the property $P$ is true if $x$ is an element of $\\underline{x}$ and false otherwise.\nThis hints a way to choose membership for all $x \\in X$, and membership is what defines a subset of $X$, thus an element of $\\mathcal{P}(X)$.\nWe define our special element, $\\underline{x}^*$, as follows:\n\\begin{align} \\forall x \\in X: x \\in \\underline{x}^* :\\iff x \\notin f(x) \\end{align} But that boils down to:\n\\begin{align} \\forall x \\in X: P(x, \\underline{x}^*) \\neq P(x, f(x)) \\end{align} Which is, by the diagonal argument, a contradiction to the assumption that $f$ is surjective. $\\blacksquare$\nThe halting problem This topic is treated a little less formal (***), but doing differently would be beyond scope.\nThe halting problem is the problem of determining, given a program and an input, if that program will halt and yield some result, or run forever.\nWe will call such a fictional program the test, and it would be tremendously useful to have such a program, because we could abort program execution before ‘accidentally’ running into an infinite loop.\nThinking about this from a philosophical perspective will soon run yield contradictions, and for me this is part of my fascination for theoretical computer science, especially since the non-existence of such a test isn’t mere ‘philosophy’, it is just not existent.\nThis has applications in static program analysis (e.g. type inference, but was relevant in understanding the PCP, and gives bounds on how far we can determine compliance of a program given a certain specification.\nOne might argue that this is all pretty old stuff, but so is a lot of ‘pure’ theory 1.\nThe basic knowledge needed to follow this example is that all programs and also their inputs could be binary encoded, and that binary encoding boils down to $\\mathbb{N}$.\nSo, again, to connect to the inital symbols of the diagonal argument - this time used in its contrapositional form:\n$A$ is the assumption that a test program $s'$ exists. $I \\equiv \\mathbb{N}$, the set of all inputs $S \\equiv \\mathbb{N}$, the set of all programs (**). $f$ is the identity function on $\\mathbb{N}$, $f(i)=i$, so we won’t explicitly mention it. $T \\equiv \\{halt,loop\\}$. $P(i,s)$ is the property that program $s$ terminates on input $i$. N.B.: we know that $f$ is surjective. It is important to understand that the assumption $A$ does not contradict the existence of $P$, which is a property of any program that holds or does not hold - no questions asked / no relevance about how to algorithmically determine that. It is the existence of an actual program $s'$ that does just that, which is in question.\nUsing the assumption $A$, the existence of a halting test $s'$, we could construct a program $s^*$ that behaves different with regards to $P$ than any other program $s$.\nThat is, we call $s'$ as a ‘subroutine’, which is the informal part (***) I mentioned earlier, and then decide to loop forever if the test yields “halt”, and we halt if the test yields “loop forever” - something like that, assuming strings are just binary input and could be mapped to natural numbers:\nfunc s_star(input string) string { if halt_test(input) == \"halt\" { for { // looping forever } } else { return \"halt\" } } Mathematically, we would express that as:\n\\begin{align} P(i, s^*) = \\begin{cases} halt \u0026 \\text{if } P(i,i) = loop \\\\ loop \u0026 \\text{if } P(i,i) = halt \\end{cases} \\end{align} So, again, by construction:\n\\begin{align} \\forall i \\in I: P(i, s^*) \\neq P(i, i) \\end{align} So, by the diagonalization argument, $f$ would not be surjective - but it is, which contradicts the assumption $A$, the existence of a test program $s'$ which we used in the construction of $s^*$. $\\blacksquare$\nOne of the main reasons for this blog is, that I see people wasting their time on problems that have been solved a long time ago… like re-inventing their own context-free parsing - and after some weeks or months of work nobody knew if that ‘spec’ even yielded a deterministic grammar -\u003e Using some standard tools like ANLR, Bison or CUP would have saved them a lot of time. Another practical experience: TDOA location is a problem with non-unique solutions (e.g. in 2D, given a 3-sensor-array). With some disturbances in the signal a multi-sensor array might yield non-unique solutions as well. These minimal solutions usually differ - but finding the ‘best’ one also means finding all of them, which is computationally expensive. Some people I got to know in my career found that out the hard way… ↩︎",
    "description": "How did I get here? Some 20 years ago, when I was an undergraduate (CS) student, I was pretty fond of theoretical computer science, but I also secretly joined fellow students in their math classes, but also lead me to pursue mathematics later…\nDuring that time I usually earned some extra money as a tutor for theoretical CS and mathematics.\nBack then I often observed that both students and teachers, struggled to explain the technique of the diagonal argument in a general sense.",
    "tags": [],
    "title": "The Diagonalization Argument",
    "uri": "/mathematics/diagonalization_argument/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming \u003e Computer Science",
    "content": "About The Golang community is rather conservative, most smaller projects just get along with go build, some larger projects including Kubernetes, Prometheus, Terraform and Vault stick with the good old Makefile. But, that being said, a new player gets some attention, Mage and I would like to introduce it here and give some reasons why I like it. I also try to be fair and give some pros and cons for both Mage and Make tools, as I’m generally no fan of black-and-white pictures - but the article deals with Mage. What we show here is:\nRun a minimal Mage example; show how to install and run the necessary tools. Debug that example using the delve debugger. Try to refute common arguments against checking in executables, although I share some scepticism about that myself. Disclaimer Warning I don’t take responsibility of any form, neither legally nor financially, for using or downloading any software, algorithms and thoughts mentioned in this article. I don’t even claim correctness of fitness for a particular purpose.\nTL;DR GNU Make Is a battle tested tool, e.g.: the Linux kernel is built with it, it is still common today to use Make with C, C++, LaTeX and generally in the DevOps and scientific computing context. Many people know how to use it; if a new member joins ’the team’, chances are much higher her/him has heard of Make than of Mage. It comes with a lot of infrastructure in and around it: Implicit rules specifically for the C language. The Autotools chain. There is a lot of (inviting) freedom to use shell, perl etc. within the targets recipes, which could be cool, but also can turn out to be nuisance. Mage Although not as standard as Make, big projects such as Hugo and Beats already adopted it. It is ’easy’ to debug a makefile, compared to Make (SO post ’tool for debugging makefiles’) - but that perception of ’easy’ might be subjective / just my view. The freedom to use any Bash-/Perl-/Linux-tool in a Make recipe imho comes at a cost. I prefer to have a typed programming language with compile time checks and a debugger at hand. This is an advantage for Go programmers, and a minus for non-Go-programmers - the ‘common’ Make recipe line relies on bash or sh, which are a very common tools. Imho the ‘compile time checks’ need some extra mentioning: it is actually a whole toolchain we get for free; linters (SonarLint), pretty printers (go fmt), unit test etc. All of this is inherently difficult to achieve with the syntactic freedom of makefiles. The idea to be able to generate a self-sufficient build executable without any dependencies appeals to me. I know the fraction of programmers who strictly refuse to check in any binaries into a repo, but with the aid of some signing and comparably small executables these are concerns which have, in my opinion, less impact today. Outside of FOSS / inside companies, it is not that uncommon to even check-in whole compiler suites just with the goal in mind to have 100% binary reproducible builds and be completely independent of any changes happening in the www. Git LFS also lowers the performance cost of doing so. You might already feel that there are some philosophical aspects lingering around here - but I guess that is no surprise when diving into a debate that basically falls into the ‘old-vs-new’ category; again - I’m not arguing globally towards new tools per se, just trying to highlight what makes Mage appealing to me.\nWalkthrough Example In this article we would like to write a small ‘magefile’, compile and debug it - and ssh-sign the final executable.\nPrerequisites At first I have to confess that I don’t know much about Windows - so this whole article was written using a Linux OS. I’m not saying Windows is a bad thing, and I guess that whole procedure could be done on a Windows OS as well, but I don’t own a Windows installation, and, although I have to use Windows at my job, this blog is entirely a spare-/fun-time-thing for me to do…\nI guess it is fully sufficient to use your favourite linux package manager and install a newer version of Golang. The official Golang installation instructions recommend removing stuf in /usr/local/go… that might be feasible, but I won’t recommend removing anything in my blog. The version I used was (thank you Manjaro-Team and pacman):\n$ go version go version go1.23.1 linux/amd64 Please make sure that ~/go/bin is part of your users PATH variable, because the tools we are about to install will reside there; You could use mkdir -p ~/go/bin and echo 'export PATH=$PATH:~/go/bin' \u003e\u003e ~/.bashrc to achieve that (probable also source ~/.bashrc).\nThe following snippets are partly taken from magefile.org. We create a build directory for Mage, i.e. ~/mage, and clone the official mage repo into it:\n$ cd \u0026\u0026 pwd /home/username $ mkdir mage $ cd mage $ git clone https://github.com/magefile/mage . The following will compile Mage and add a binary called ‘mage’ into ~/go/bin\n# inside ~/mage we run to compile and install the mage executable under ~/go/bin: $ go run bootstrap.go # because ~/go/bin/ is part of my PATH variable we can call mage directly: $ which mage /home/username/go/bin/mage $ mage --version # your version build number might differ of course Mage Build Tool v1.15.0-5-g2385abb Build Date: 2024-10-22T14:28:02+02:00 Commit: 2385abb built with: go1.23.1 Later in this tutorial we will need the delve debugger - possibly also an IDE to squeeze the last drop of fanciness out of this article, but I’ll keep that part optional.\nTaken from https://github.com/go-delve/delve/tree/master/Documentation/installation:\n$ cd \u0026\u0026 pwd /home/username $ mkdir delve \u0026\u0026 cd delve $ git clone https://github.com/go-delve/delve . # we then run go install to compile and install the dlv executable under ~/go/bin: $ go install github.com/go-delve/delve/cmd/dlv $ which dlv /home/username/go/bin/dlv $ dlv version # your version build number might differ of course Delve Debugger Version: 1.23.1 Build: $Id: 2eba762d75437d380e48fc42213853f13aa2904d Now we are ready for…\nWriting Our First Magefile $ cd \u0026\u0026 pwd /home/username $ mkdir hello-mage $ cd hello-mage $ go mod init hello-mage $ touch magefile.go $ ls -lh total 8.0K -rw-r--r-- 1 peterpan peterpan 29 Oct 28 13:22 go.mod -rw-r--r-- 1 peterpan peterpan 574 Oct 22 15:14 magefile.go We then grab our favourite editor and fill magefile.go with some content:",
    "description": "About The Golang community is rather conservative, most smaller projects just get along with go build, some larger projects including Kubernetes, Prometheus, Terraform and Vault stick with the good old Makefile. But, that being said, a new player gets some attention, Mage and I would like to introduce it here and give some reasons why I like it. I also try to be fair and give some pros and cons for both Mage and Make tools, as I’m generally no fan of black-and-white pictures - but the article deals with Mage. What we show here is:",
    "tags": [],
    "title": "Mage",
    "uri": "/computer-science/mage/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Mathematics and Programming",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
